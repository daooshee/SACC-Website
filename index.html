<!DOCTYPE HTML>
<html class="no-js">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <title>HLA-Face</title>
    <meta name="description" content="Lithium Description" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <link href="css/plugins.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/application.css" media="screen" rel="stylesheet" type="text/css" />
  </head>

<body>

    <!-- ABOUT -->

    <section id="page-about" class="section">
      <div align="center" style="padding-bottom: 100px;">
        <p class="copy-02">ACM MM 2022</p>
        <p class="heading h-01">Self-Aligned Concave Curve: Illumination Enhancement for Unsupervised Adaptation</p>

        <p class="copy-02">
          <a href="https://daooshee.github.io/website/">Wenjing Wang</a> &nbsp;&nbsp;&nbsp;
          <a href="https://iceyxxx.github.io/">Zhengbo Xu</a> &nbsp;&nbsp;&nbsp;
          <a href="https://huangerbai.github.io/">Haofeng Huang</a> &nbsp;&nbsp;&nbsp;
          <a href="https://www.icst.pku.edu.cn/struct/people/liujiaying.html">Jiaying Liu</a>
        </p>
      </div>

      <div class="site-inner">
        <h3 class="heading h-03">Abstract</h3>
            <p class="copy-02">Low light conditions not only degrade human visual experience, but also reduce the performance of downstream machine analytics. Although many works have been designed for low-light enhancement or domain adaptive machine analytics, the former considers less on high-level vision, while the latter neglects the potential of image-level signal adjustment. How to restore underexposed images/videos from the perspective of machine vision has long been overlooked. In this paper, we are the first to propose a learnable illumination enhancement model for high-level vision. Inspired by real camera response functions, we assume that the illumination enhancement function should be a concave curve, and propose to satisfy this concavity through discrete integral. With the intention of adapting illumination from the perspective of machine vision without task-specific annotated data, we design an asymmetric cross-domain self-supervised training strategy. Our model architecture and training designs mutually benefit each other, forming a powerful unsupervised normal-to-low light adaptation framework. Comprehensive experiments demonstrate that our method surpasses existing low-light enhancement and adaptation methods and shows superior generalization on various low-light vision tasks, including classification, detection, action recognition, and optical flow estimation. All of our data, code, and results will be available online upon publication of the paper.</p>
      </div>

<!--       <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">Method</h3>
        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="motivation.jpg" width=90%> <br>
        </div>

        <p class='copy-02'>Motivation: comparison of different adaptive low light detection techniques. L: low light data. H: normal light data. Existing enhancement-based, darkening-based, and feature adaptation methods either ignore the high-level gap, or have limited effects due to the huge and complex gap between L and H. Our method instead considers both low-level and high-level adaptation, therefore achieves better performance.
        </p>

        <br><br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="framework_2.jpg" width=70%> <br>
        </div>

        <p class='copy-02'>Framework: <a style="color:#4C6E8B"><b>LOW-LEVEL</b></a> adaptation fills the gap by creating intermediate states. We bidirectionally brighten the low light data as well as distort the normal light data with noise and color bias. Based on the built intermediate states, we use multi-task cross-domain self-supervised learning to fill the <a style="color:#85937E"><b>HIGH-LEVEL</b></a> gap.
        </p>
      </div> -->

<!--       <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">Selected Experimental Results</h3>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="comp_map.jpg" width=60%> <br>
        </div>

        <div align="center">
          <p class='copy-02'>Precision-Recall (PR) curves on DARK FACE.
          </p>
        </div>

        <br><br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="enh_comp.jpg" width=90%> <br>
        </div>



        <p class='copy-02'>Qualitative comparison of different enhancement-based methods. (a) Input low light image and the ground truth boxes. (b)-(g) Results of low-light enhancement methods with DSFD [1]. (h) Our result.
        </p>

      </div>
 -->


      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <ul style="line-height:1.5; padding-left: 50px; padding-right: 50px">
          　　<li class="copy-02"> Paper: Coming Soon
            <!-- <a href="https://arxiv.org/abs/2104.01984">arXiv</a></li> -->
          　　<li class="copy-02"> Code: Coming Soon
            <!-- <a href="https://github.com/daooshee/HLA-Face-Code">Github</a></li> -->
          　　<li class="copy-02"> Supplementary Material: Coming Soon
            <!-- <a href="https://github.com/daooshee/HLA-Face-Code/blob/main/Supplementary%20Material.pdf">PDF</a></li> -->
          </ul>
      </div>
      

<!--       <div class="site-inner" style="padding-top:50px;">
        <p class='heading h-03'> Citation</p>
        <p class="copy-02"> @InProceedings{HLAFace_2021_CVPR, <br>
        &nbsp; &nbsp; author = {Wang, Wenjing and Yang, Wenhan and Liu, Jiaying}, <br>
        &nbsp; &nbsp; title = {HLA-Face: Joint High-Low Adaptation for Low Light Face Detection}, <br>
        &nbsp; &nbsp; booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
        &nbsp; &nbsp; month = {June}, <br>
        &nbsp; &nbsp; year = {2021} <br>
        } <br> 
        </p>
      </div>
 -->
<!--       <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <p class="copy-02"> [1] Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie Wang, Jilin Li, Feiyue Huang: DSFD: Dual Shot Face Detector. CVPR 2019: 5060-5069</p>
          <br>
          <p class="copy-02"> [2] Xiaojie Guo, Yu Li, Haibin Ling: LIME: Low-Light Image Enhancement via Illumination Map Estimation. IEEE Trans. Image Process. 26(2): 982-993 (2017)</p>
      </div>
 -->
    <section id="page-about" class="section">

</body>
</html>